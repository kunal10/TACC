{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import logging\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PyPDF2 as pyPdf\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim import models\n",
    "from nltk.corpus import stopwords\n",
    "# Needed only if you want to remove stop words.\n",
    "# nltk.download()\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Configurable parameters\n",
    "CATEGORY_FILE = 'categories'\n",
    "EXPERIMENT_FILE = 'test_document'\n",
    "DATA_DIR = './data'\n",
    "\n",
    "# Logging level\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_document(filename):\n",
    "    \"\"\" Read input pdf file and parse text from it \n",
    "        TODO : Some words are not space separated \n",
    "    \"\"\"\n",
    "    if not filename.endswith(\".pdf\"):\n",
    "        raise Exception('Input file is not in pdf format !!')\n",
    "    text = []\n",
    "    pdf = pyPdf.PdfFileReader(open(filename, \"rb\"))\n",
    "    for page in pdf.pages:\n",
    "        raw_text = page.extractText()\n",
    "        lines = re.split('\\n', raw_text)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CategoryMatcher(object):\n",
    "    \"\"\" Category matcher built from existing categories file.\n",
    "        Can be used to match categories for terms.\n",
    "        TODO : Handle variations like phrases, typos etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, category_file, data_dir):\n",
    "        self.category_file = category_file\n",
    "        self.data_dir = data_dir\n",
    "        # Dict to store categories for a given term.\n",
    "        # TODO: Conisder using Prefix Tree instead of Dictionary.\n",
    "        self.term_categories = {}\n",
    "        # Dict to store terms for a given category.\n",
    "        self.category_terms = {}\n",
    "        self.categories = set([])\n",
    "        self.is_built = False\n",
    "\n",
    "    def get_categories(self):\n",
    "        return self.categories\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_text(text):\n",
    "        if text is None:\n",
    "            return text\n",
    "        # Remove non-alphabetic characters\n",
    "        text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "        # Convert to lower case\n",
    "        text = text.lower()\n",
    "        # Strip leading and ending whitespace\n",
    "        text = text.strip()\n",
    "        return text\n",
    "    \n",
    "    def add_term_category(self, term, category):\n",
    "        self.category_terms[category].add(term)\n",
    "        if term not in self.term_categories:\n",
    "            self.term_categories[term] = set([category])\n",
    "        else:\n",
    "            self.term_categories[term].add(category)\n",
    "    \n",
    "    def build(self):\n",
    "        \"\"\" Parses the category file and builds a set of categories for each term.\n",
    "        \n",
    "            Assumes following format for category file:\n",
    "            # Comments(if any) about category1\n",
    "            Category1,Word1,Word2 ...\n",
    "            # Comments(if any) about category1\n",
    "            Category2,Word1,Word2 ...\n",
    "            .\n",
    "            .\n",
    "            .\n",
    "            \n",
    "            NOTE: One term can have multiple categories\n",
    "            TODO: Generate this file automatically from ontology.\n",
    "        \"\"\"\n",
    "        with open(self.category_file, 'rb') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                # Skip blank lines\n",
    "                if len(line) == 0:\n",
    "                    continue\n",
    "                # Skip comments\n",
    "                if line.startswith('#'):\n",
    "                    continue\n",
    "                    \n",
    "                terms = line.split(',')\n",
    "                category = self.process_text(terms[0])\n",
    "                if category not in self.categories:\n",
    "                    self.categories.add(category)\n",
    "                    self.category_terms[category] = set([])\n",
    "                for term in terms[1:]:\n",
    "                    term = self.process_text(term)\n",
    "                    self.add_term_category(term, category)\n",
    "        self.is_built = True\n",
    "        \n",
    "    def get_term(self, words, index):\n",
    "        \"\"\" Returns longest n-gram starting at given index, \n",
    "            which is present in currently seen terms\n",
    "        \"\"\"\n",
    "        num_words = len(words)\n",
    "        # Computer gram, bigram and trigram\n",
    "        gram, bigram, trigram = self.process_text(words[index]), None, None\n",
    "        if (index + 1) < num_words:\n",
    "            bigram = ' '.join([gram, self.process_text(words[index + 1])])\n",
    "        if (index + 2) < num_words:\n",
    "            trigram = ' '.join([bigram, self.process_text(words[index + 2])])\n",
    "        \n",
    "        # Return largest matching n-gram\n",
    "        if trigram in self.term_categories:\n",
    "            return trigram\n",
    "        elif bigram in self.term_categories:\n",
    "            return bigram\n",
    "        else:\n",
    "            return gram\n",
    "        \n",
    "    # TODO : Use NLTK phrase detection for non ontology\n",
    "    def get_terms(self, line):\n",
    "        terms = []\n",
    "        words = line.strip().split()\n",
    "        i = 0\n",
    "        while i < len(words):\n",
    "            term = self.get_term(words, i)\n",
    "            num_words = len(term.split())\n",
    "            if num_words == 0:\n",
    "                # Sometimes returned term is empty. So term.split is list of size 0\n",
    "                i += 1\n",
    "            else:\n",
    "                i += num_words\n",
    "            terms.append(term)\n",
    "        return terms\n",
    "    \n",
    "    # TODO : Make this flexible to return top k categories\n",
    "    def get_representative_category(self, terms):\n",
    "        category_count = {}\n",
    "        for category in self.categories:\n",
    "            category_count[category] = 0\n",
    "        for term in terms:\n",
    "            if term not in self.term_categories:\n",
    "                continue\n",
    "            for category in self.term_categories[term]:\n",
    "                category_count[category] += self.get_tf_idf(category, term)\n",
    "        return max(category_count, key=category_count.get)\n",
    "    \n",
    "    def process_paragraph(self, lines):\n",
    "        # TODO: Figure out appropriate context window length\n",
    "        # for finding representative category. Currently we \n",
    "        # are using sentences.\n",
    "        for line in lines:\n",
    "            terms = self.get_terms(line)\n",
    "            category = self.get_representative_category(terms)\n",
    "            for term in terms:\n",
    "                self.add_term_category(term, category)\n",
    "                \n",
    "    def update(self):\n",
    "        for category_dir in os.listdir(self.data_dir):\n",
    "            logger.info(\"Processing Category: {0}\".format(category_dir))\n",
    "            documents = os.listdir(os.path.join(self.data_dir, category_dir))\n",
    "            for document in documents:\n",
    "                if not document.endswith(\".pdf\"):\n",
    "                    print(document, ' is not a pdf. Skipping')\n",
    "                    continue\n",
    "                logger.info(\"Processing Document: {0}\".format(document))\n",
    "                lines = read_document(os.path.join(self.data_dir, category_dir, document))\n",
    "                paragraph_length = 4\n",
    "                num_paragraphs = len(lines)/paragraph_length\n",
    "                for i in range(num_paragraphs):\n",
    "                    start = i * paragraph_length\n",
    "                    end = min((i+1) * paragraph_length, len(lines) + 1)\n",
    "                    self.process_paragraph(lines[start:end])\n",
    "                \n",
    "    def get_term_categories(self, term):\n",
    "        \"\"\" Returns set of categories for given term\n",
    "        Attributes\n",
    "            term -- Word for which we want to retrieve categories.\n",
    "        Returns\n",
    "            Set of categories given term belongs to.\n",
    "        \"\"\"\n",
    "        if not self.is_built:\n",
    "            raise Exception('Category Matcher not built !!')\n",
    "        term = self.process_text(term)\n",
    "        if term in self.term_categories:\n",
    "            return self.term_categories[term]\n",
    "        # Return empty set if term categories are not present.\n",
    "        logger.debug('\"{0}\" not present. Returning empty set'.format(term))\n",
    "        return set([])\n",
    "    \n",
    "    def get_tf(self, category, word):\n",
    "        \"\"\" Returns term frequency word in given category \n",
    "        Attributes\n",
    "            word -- Word for which TF has to be computed\n",
    "            category -- Category wrt which TF is computed\n",
    "        Returns\n",
    "            Term Frequencency for word wrt given category.\n",
    "        \"\"\"\n",
    "        tf = 0\n",
    "        total = 0\n",
    "        for term in self.category_terms[category]:\n",
    "            for term_word in term.split(' '):\n",
    "                total += 1\n",
    "                if word == term_word:\n",
    "                    tf += 1\n",
    "        return tf/total\n",
    "    \n",
    "    def get_idf(self, term):\n",
    "        \"\"\" Returns IDF of term across all categories\n",
    "        Attributes\n",
    "            term -- Term for which IDF has to be computed\n",
    "        Returns\n",
    "            Inverse document frequency of term across all categories.\n",
    "        \"\"\"\n",
    "        num_term_categories = 0\n",
    "        # Term contains a single word.\n",
    "        if len(term.split(' ')) == 1: \n",
    "            for category in self.categories:\n",
    "                is_present = False\n",
    "                for term in self.category_terms[category]:\n",
    "                    # Term can be multiple words. \n",
    "                    # So match word to all words in term.\n",
    "                    for term_word in term.split(' '):\n",
    "                        if term == term_word:\n",
    "                            num_term_categories += 1\n",
    "                            is_present = True\n",
    "                            break\n",
    "                    if (is_present):\n",
    "                        break\n",
    "        else: # Term contains multiple words\n",
    "            # TODO: Consider multiple word level match,\n",
    "            # like we do for individual words above.\n",
    "            num_term_categories = self.get_term_categories(term)\n",
    "            \n",
    "        if num_term_categories == 0:\n",
    "            raise Exception('IDF called for a term not present in Category Matcher')\n",
    "        num_categories = len(self.categories)\n",
    "        return np.log(num_categories / num_term_categories)\n",
    "    \n",
    "    def get_tf_idf(self, category, word):\n",
    "        \"\"\" Returns TF-IDF of term across all categories\n",
    "        Attributes\n",
    "            word -- Word for which TF-IDF score is to be computed\n",
    "            category -- Category wrt to which TF is computed.\n",
    "        Returns\n",
    "            Inverse document frequency of term across all categories.\n",
    "        \"\"\"\n",
    "        return self.get_tf(category, word) * self.get_idf(word) \n",
    "        \n",
    "    def get_category_vector(self, term):\n",
    "        \"\"\" Returns a category vector representation for give term.\n",
    "        Attributes\n",
    "            term -- Word for which we want to category vector\n",
    "        Returns\n",
    "            Vector correponding to categories of given term.\n",
    "        \"\"\"\n",
    "        term_categories = self.get_term_categories(term)\n",
    "        category_vector = [category in term_categories for category in sorted(self.categories)]\n",
    "        return np.array(category_vector, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CategoryPredictor(object):\n",
    "    \"\"\" Categorry Predictor for documents. Learnt from training data \"\"\"\n",
    "    \n",
    "    def __init__(self, category_file, data_dir):\n",
    "        self.category_matcher_ = CategoryMatcher(category_file, data_dir)\n",
    "        self.category_matcher_.build()\n",
    "        self.raw_labels_ = list(self.category_matcher_.get_categories())\n",
    "        self.label_encoder_ = preprocessing.LabelEncoder()\n",
    "        self.data_dir_ = data_dir\n",
    "        self.X_train_ = []\n",
    "        self.y_train_ = []\n",
    "        \n",
    "    @staticmethod\n",
    "    def read_document(filename):\n",
    "        \"\"\" Read input pdf file and parse text from it \n",
    "            TODO : Some words are not space separated \n",
    "        \"\"\"\n",
    "        if not filename.endswith(\".pdf\"):\n",
    "            raise Exception('Input file is not in pdf format !!')\n",
    "        text = []\n",
    "        pdf = pyPdf.PdfFileReader(open(filename, \"rb\"))\n",
    "        for page in pdf.pages:\n",
    "            raw_text = page.extractText()\n",
    "            raw_text = re.split('\\W+', raw_text)\n",
    "            clean_text = [word.strip() for word in raw_text]\n",
    "            text = text + clean_text\n",
    "        # text = list(text)\n",
    "        return ' '.join(text)\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess(text, remove_stopwords = True):\n",
    "        \"\"\" Function to convert raw text to a sequence of words,\n",
    "            optionally removing stop words. Returns a list of words.\n",
    "        \"\"\"\n",
    "        text = text.strip()\n",
    "        if len(text) == 0:\n",
    "            return text\n",
    "        \n",
    "        # 1. Remove HTML\n",
    "        text = BeautifulSoup(text).get_text()\n",
    "          \n",
    "        # 2. Remove \\n and non-digit characters.\n",
    "        # TODO: Revisit this to see what characters should be kept.\n",
    "        # Probably numbers, -, _ etc\n",
    "        text = re.sub(r\"\\\\n\",\" \", text)\n",
    "        text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "        \n",
    "        # 3. Convert words to lower case and split them\n",
    "        words = text.lower().split()\n",
    "        \n",
    "        # 4. Optionally remove stop words (false by default)\n",
    "        if remove_stopwords:\n",
    "            stops = set(stopwords.words('english'))\n",
    "            words = [w for w in words if not w in stops]\n",
    "        \n",
    "        # 5. Join the words back into one string separated by space, \n",
    "        # and return the result.\n",
    "        return (\" \".join(words))\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        self.label_encoder_.fit(self.raw_labels_)\n",
    "        logger.debug('Training Category Predictor for following categories:\\n{0}'\n",
    "                     .format('\\t'.join(self.raw_labels_)))\n",
    "        \n",
    "        for category_dir in os.listdir(self.data_dir_):\n",
    "            logger.info(\"Processing Category: {0}\".format(category_dir))\n",
    "            documents = os.listdir(os.path.join(self.data_dir_, category_dir))\n",
    "            for document in documents:\n",
    "                raw_text = self.read_document(os.path.join(self.data_dir_, category_dir, document))\n",
    "                # This might not be required if CountVectorizer provides \n",
    "                # all required processing capabilities\n",
    "                clean_text = self.preprocess(raw_text)\n",
    "                self.X_train_.append(clean_text)\n",
    "                self.y_train_.append(self.label_encoder_.transform([category_dir])[0])\n",
    "                \n",
    "    def get_document_vector(self, document_dir, document):\n",
    "        raw_text = self.read_document(os.path.join(self.data_dir_, category_dir, document))\n",
    "        # Consider using count vectorizer for this.\n",
    "        clean_text = self.preprocess(raw_text)\n",
    "                \n",
    "    def train_model(self):\n",
    "        pipeline = Pipeline([\n",
    "            ('vect', TfidfVectorizer(analyzer='word', stop_words='english',\n",
    "                                     strip_accents='unicode', ngram_range=(1,2))),\n",
    "            ('clf', SGDClassifier(penalty='l2'))\n",
    "        ])\n",
    "\n",
    "        # Uncommenting more parameters will give better exploring power but will\n",
    "        # increase processing time in a combinatorial way\n",
    "        parameters = {\n",
    "            'vect__max_df': (0.5, 0.75, 1.0),\n",
    "            #'vect__max_features': (None, 5000, 10000, 50000),\n",
    "            #'tfidf__use_idf': (True, False),\n",
    "            #'tfidf__norm': ('l1', 'l2'),\n",
    "            'clf__alpha': (0.00001, 0.000001),\n",
    "#             'clf__penalty': ('l2', 'elasticnet'),\n",
    "            #'clf__n_iter': (10, 50, 80),\n",
    "        }\n",
    "                \n",
    "        # Find the best parameters for both the feature extraction and the\n",
    "        # classifier\n",
    "        grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=0)\n",
    "\n",
    "        print(\"Performing grid search...\")\n",
    "        print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "        print(\"parameters:\")\n",
    "        pprint(parameters)\n",
    "        t0 = time()\n",
    "        grid_search.fit(self.X_train_, self.y_train_)\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        print()\n",
    "\n",
    "        print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "        print(\"Best parameters set:\")\n",
    "        best_parameters = grid_search.best_estimator_.get_params()\n",
    "        for param_name in sorted(parameters.keys()):\n",
    "            print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    category_matcher = CategoryMatcher(CATEGORY_FILE, DATA_DIR)\n",
    "    category_matcher.build()\n",
    "#     with open(EXPERIMENT_FILE, 'rb') as f:\n",
    "#         for line in f:\n",
    "#             for term in line.split(' '):\n",
    "#                 categories = category_matcher.get_term_categories(term)\n",
    "#                 logger.debug(term, len(categories))\n",
    "#     category_predictor = CategoryPredictor(CATEGORY_FILE, DATA_DIR)\n",
    "#     category_predictor.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'centrifuge': set(['centrifuge']), 'force': set(['centrifuge', 'earthquake']), 'pressure': set(['centrifuge']), 'magnitude': set(['earthquake']), 'rotator': set(['centrifuge']), 'centrifuge pump': set(['centrifuge']), 'richter': set(['earthquake']), 'earthquake': set(['earthquake'])}\n"
     ]
    }
   ],
   "source": [
    "print(category_matcher.term_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'centrifuge': set(['centrifuge', 'pressure', 'centrifuge pump', 'force', 'rotator']), 'earthquake': set(['force', 'magnitude', 'richter', 'earthquake'])}\n"
     ]
    }
   ],
   "source": [
    "print(category_matcher.category_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing Category: centrifuge\n",
      "INFO:root:Processing Document: Experimental Methods.pdf\n",
      "INFO:root:Processing Category: earthquake\n",
      "INFO:root:Processing Document: Experimental Methods.pdf\n",
      "INFO:root:Processing Category: TriaxialLabData\n",
      "INFO:root:Processing Document: Experimental Methods.pdf\n"
     ]
    }
   ],
   "source": [
    "category_matcher.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'centrifuge': {u'',\n",
       "  u'a',\n",
       "  u'after',\n",
       "  u'and',\n",
       "  u'ared',\n",
       "  u'as',\n",
       "  u'at',\n",
       "  u'back',\n",
       "  u'be',\n",
       "  u'by',\n",
       "  u'can',\n",
       "  u'case',\n",
       "  'centrifuge',\n",
       "  'centrifuge pump',\n",
       "  u'closed',\n",
       "  u'computer',\n",
       "  u'conditions',\n",
       "  u'configuration',\n",
       "  u'configured',\n",
       "  u'consolidation',\n",
       "  u'constant',\n",
       "  u'controlled',\n",
       "  u'defined',\n",
       "  u'drainage',\n",
       "  u'drained',\n",
       "  u'during',\n",
       "  u'either',\n",
       "  u'example',\n",
       "  u'figure',\n",
       "  u'for',\n",
       "  'force',\n",
       "  u'in',\n",
       "  u'initial',\n",
       "  u'intervals',\n",
       "  u'is',\n",
       "  u'it',\n",
       "  u'kept',\n",
       "  u'kpa',\n",
       "  u'loading',\n",
       "  u'manually',\n",
       "  u'note',\n",
       "  u'of',\n",
       "  u'open',\n",
       "  u'opened',\n",
       "  u'or',\n",
       "  'pressure',\n",
       "  u'process',\n",
       "  u're',\n",
       "  'rotator',\n",
       "  u'screenshots',\n",
       "  u'seen',\n",
       "  u'setup',\n",
       "  u'she',\n",
       "  u'shear',\n",
       "  u'shown',\n",
       "  u'specimen',\n",
       "  u'stage',\n",
       "  u'started',\n",
       "  u'static',\n",
       "  u'such',\n",
       "  u'test',\n",
       "  u'testing',\n",
       "  u'that',\n",
       "  u'the',\n",
       "  u'then',\n",
       "  u'under',\n",
       "  u'undrained',\n",
       "  u'universal',\n",
       "  u'usually',\n",
       "  u'value',\n",
       "  u'valve',\n",
       "  u'where',\n",
       "  u'which'},\n",
       " 'earthquake': {'earthquake', 'force', 'magnitude', 'richter'}}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_matcher.category_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'': {'centrifuge'},\n",
       " u'a': {'centrifuge'},\n",
       " u'after': {'centrifuge'},\n",
       " u'and': {'centrifuge'},\n",
       " u'ared': {'centrifuge'},\n",
       " u'as': {'centrifuge'},\n",
       " u'at': {'centrifuge'},\n",
       " u'back': {'centrifuge'},\n",
       " u'be': {'centrifuge'},\n",
       " u'by': {'centrifuge'},\n",
       " u'can': {'centrifuge'},\n",
       " u'case': {'centrifuge'},\n",
       " 'centrifuge': {'centrifuge'},\n",
       " 'centrifuge pump': {'centrifuge'},\n",
       " u'closed': {'centrifuge'},\n",
       " u'computer': {'centrifuge'},\n",
       " u'conditions': {'centrifuge'},\n",
       " u'configuration': {'centrifuge'},\n",
       " u'configured': {'centrifuge'},\n",
       " u'consolidation': {'centrifuge'},\n",
       " u'constant': {'centrifuge'},\n",
       " u'controlled': {'centrifuge'},\n",
       " u'defined': {'centrifuge'},\n",
       " u'drainage': {'centrifuge'},\n",
       " u'drained': {'centrifuge'},\n",
       " u'during': {'centrifuge'},\n",
       " 'earthquake': {'earthquake'},\n",
       " u'either': {'centrifuge'},\n",
       " u'example': {'centrifuge'},\n",
       " u'figure': {'centrifuge'},\n",
       " u'for': {'centrifuge'},\n",
       " 'force': {'centrifuge', 'earthquake'},\n",
       " u'in': {'centrifuge'},\n",
       " u'initial': {'centrifuge'},\n",
       " u'intervals': {'centrifuge'},\n",
       " u'is': {'centrifuge'},\n",
       " u'it': {'centrifuge'},\n",
       " u'kept': {'centrifuge'},\n",
       " u'kpa': {'centrifuge'},\n",
       " u'loading': {'centrifuge'},\n",
       " 'magnitude': {'earthquake'},\n",
       " u'manually': {'centrifuge'},\n",
       " u'note': {'centrifuge'},\n",
       " u'of': {'centrifuge'},\n",
       " u'open': {'centrifuge'},\n",
       " u'opened': {'centrifuge'},\n",
       " u'or': {'centrifuge'},\n",
       " 'pressure': {'centrifuge'},\n",
       " u'process': {'centrifuge'},\n",
       " u're': {'centrifuge'},\n",
       " 'richter': {'earthquake'},\n",
       " 'rotator': {'centrifuge'},\n",
       " u'screenshots': {'centrifuge'},\n",
       " u'seen': {'centrifuge'},\n",
       " u'setup': {'centrifuge'},\n",
       " u'she': {'centrifuge'},\n",
       " u'shear': {'centrifuge'},\n",
       " u'shown': {'centrifuge'},\n",
       " u'specimen': {'centrifuge'},\n",
       " u'stage': {'centrifuge'},\n",
       " u'started': {'centrifuge'},\n",
       " u'static': {'centrifuge'},\n",
       " u'such': {'centrifuge'},\n",
       " u'test': {'centrifuge'},\n",
       " u'testing': {'centrifuge'},\n",
       " u'that': {'centrifuge'},\n",
       " u'the': {'centrifuge'},\n",
       " u'then': {'centrifuge'},\n",
       " u'under': {'centrifuge'},\n",
       " u'undrained': {'centrifuge'},\n",
       " u'universal': {'centrifuge'},\n",
       " u'usually': {'centrifuge'},\n",
       " u'value': {'centrifuge'},\n",
       " u'valve': {'centrifuge'},\n",
       " u'where': {'centrifuge'},\n",
       " u'which': {'centrifuge'}}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_matcher.term_categories"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
