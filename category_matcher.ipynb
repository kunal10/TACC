{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import logging\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PyPDF2 as pyPdf\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim import models\n",
    "from nltk.corpus import stopwords\n",
    "# Needed only if you want to remove stop words.\n",
    "# nltk.download()\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Configurable parameters\n",
    "CATEGORY_FILE = 'categories'\n",
    "EXPERIMENT_FILE = 'e1'\n",
    "DATA_DIR = './data'\n",
    "\n",
    "# Logging level\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_document(filename):\n",
    "    \"\"\" Read input pdf file and parse text from it \n",
    "        TODO : Some words are not space separated \n",
    "    \"\"\"\n",
    "    if not filename.endswith(\".pdf\"):\n",
    "        raise Exception('Input file is not in pdf format !!')\n",
    "    text = []\n",
    "    pdf = pyPdf.PdfFileReader(open(filename, \"rb\"))\n",
    "    for page in pdf.pages:\n",
    "        raw_text = page.extractText()\n",
    "        lines = re.split('\\n', raw_text)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CategoryMatcher(object):\n",
    "    \"\"\" Category matcher built from existing categories file.\n",
    "        Can be used to match categories for terms.\n",
    "        TODO : Handle variations like phrases, typos etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, category_file, data_dir):\n",
    "        self.category_file = category_file\n",
    "        self.data_dir = data_dir\n",
    "        # Dict to store categories for a given term.\n",
    "        # TODO: Conisder using Prefix Tree instead of Dictionary.\n",
    "        self.term_categories = {}\n",
    "        # Dict to store terms for a given category.\n",
    "        self.category_terms = {}\n",
    "        self.categories = set([])\n",
    "        self.is_built = False\n",
    "\n",
    "    def get_categories(self):\n",
    "        return self.categories\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_text(text):\n",
    "        if text is None:\n",
    "            return text\n",
    "        # Strip leading and ending whitespace\n",
    "        text = text.strip()\n",
    "        # Convert to lower case\n",
    "        text = text.lower()\n",
    "        # Remove non-alphabetic characters\n",
    "        text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "        return text\n",
    "    \n",
    "    def add_term_category(self, term, category):\n",
    "        self.category_terms[category].add(term)\n",
    "        if term not in self.term_categories:\n",
    "            self.term_categories[term] = set([category])\n",
    "        else:\n",
    "            self.term_categories[term].add(category)\n",
    "    \n",
    "    def build(self):\n",
    "        \"\"\" Parses the category file and builds a set of categories for each term.\n",
    "        \n",
    "            Assumes following format for category file:\n",
    "            Category1,Word1,Word2 ...\n",
    "            Category2,Word1,Word2 ...\n",
    "            .\n",
    "            .\n",
    "            .\n",
    "            \n",
    "            NOTE: One term can have multiple categories\n",
    "        \"\"\"\n",
    "        with open(self.category_file, 'rb') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                # Skip blank lines\n",
    "                if len(line) == 0:\n",
    "                    continue\n",
    "                # Skip comments\n",
    "                if line.startswith('#'):\n",
    "                    continue\n",
    "                    \n",
    "                terms = line.split(',')\n",
    "                category = self.process_text(terms[0])\n",
    "                if category not in self.categories:\n",
    "                    self.categories.add(category)\n",
    "                    self.category_terms[category] = set([])\n",
    "                for term in terms[1:]:\n",
    "                    term = self.process_text(term)\n",
    "                    self.add_term_category(term, category)\n",
    "        self.is_built = True\n",
    "        \n",
    "    def get_term(self, words, index):\n",
    "        \"\"\" Returns longest n-gram starting at given index, \n",
    "            which is present in currently seen terms\n",
    "        \"\"\"\n",
    "        num_words = len(words)\n",
    "        # Computer gram, bigram and trigram\n",
    "        gram, bigram, trigram = self.process_text(words[index]), None, None\n",
    "        if (index + 1) < num_words:\n",
    "            bigram = ' '.join([gram, self.process_text(words[index + 1])])\n",
    "        if (index + 2) < num_words:\n",
    "            trigram = ' '.join([bigram, self.process_text(words[index + 2])])\n",
    "        \n",
    "        # Return largest matching n-gram\n",
    "        if trigram in self.term_categories:\n",
    "            return trigram\n",
    "        elif bigram in self.term_categories:\n",
    "            return bigram\n",
    "        else:\n",
    "            return gram\n",
    "        \n",
    "    def get_terms(self, line):\n",
    "        terms = []\n",
    "        words = line.split()\n",
    "        i = 0\n",
    "        while i < len(words):\n",
    "#             print('Calling get term for index: %d out of %d' %(i, len(words)))\n",
    "            term = self.get_term(words, i)\n",
    "            i += len(term.split())\n",
    "            terms.append(term)\n",
    "        return terms\n",
    "    \n",
    "    def get_representative_category(self, terms):\n",
    "        category_count = {}\n",
    "        for category in self.categories:\n",
    "            category_count[category] = 0\n",
    "        for term in terms:\n",
    "            if term not in self.term_categories:\n",
    "                continue\n",
    "            for category in self.term_categories[term]:\n",
    "                category_count[category] += get_tf_idf(category, term)\n",
    "        return max(category_count, key=category_count.get)\n",
    "    \n",
    "    def process_paragraph(self, lines):\n",
    "        # TODO: Figure out appropriate context window length\n",
    "        # for finding representative category. Currently we \n",
    "        # are using sentences.\n",
    "        print('\\n\\n\\n\\nProcessing paragraph: ', lines)        \n",
    "        for line in lines:\n",
    "            print('\\nProcessing Line: ', line)\n",
    "            terms = self.get_terms(line)\n",
    "            print(len(terms), terms)\n",
    "            print('Calling get representative category for: ', terms)\n",
    "            category = self.get_representative_category(terms)\n",
    "            print('Representative category is: ', category)\n",
    "            for term in terms:\n",
    "                print('Adding term category: ', term, category)\n",
    "                self.add_term_category(term, category)\n",
    "                \n",
    "    def update(self):\n",
    "        for category_dir in os.listdir(self.data_dir):\n",
    "            if category_dir != 'TriaxialLabData':\n",
    "                continue\n",
    "            logger.info(\"Processing Category: {0}\".format(category_dir))\n",
    "            documents = os.listdir(os.path.join(self.data_dir, category_dir))\n",
    "            for document in documents:\n",
    "                if not document.endswith(\".pdf\"):\n",
    "                    print(document, ' is not a pdf. Skipping')\n",
    "                    continue\n",
    "                logger.info(\"Processing Document: {0}\".format(document))\n",
    "                lines = read_document(os.path.join(self.data_dir, category_dir, document))\n",
    "                paragraph_length = 4\n",
    "                num_paragraphs = len(lines)/paragraph_length\n",
    "                for i in range(num_paragraphs):\n",
    "                    start = i * paragraph_length\n",
    "                    end = min((i+1) * paragraph_length, len(lines) + 1)\n",
    "                    self.process_paragraph(lines[start:end])\n",
    "                \n",
    "    def get_term_categories(self, term):\n",
    "        \"\"\" Returns set of categories for given term\n",
    "        Attributes\n",
    "            term -- Word for which we want to retrieve categories.\n",
    "        Returns\n",
    "            Set of categories given term belongs to.\n",
    "        \"\"\"\n",
    "        if not self.is_built:\n",
    "            raise Exception('Category Matcher not built !!')\n",
    "        term = self.process_text(term)\n",
    "        if term in self.term_categories:\n",
    "            return self.term_categories[term]\n",
    "        # Return empty set if term categories are not present.\n",
    "        logger.debug('\"{0}\" not present. Returning empty set'.format(term))\n",
    "        return set([])\n",
    "    \n",
    "    def get_tf(self, category, word):\n",
    "        tf = 0\n",
    "        total = 0\n",
    "        for term in self.category_terms[category]:\n",
    "            for term_word in term.split(' '):\n",
    "                total += 1\n",
    "                if word == term_word:\n",
    "                    tf += 1\n",
    "        return tf/total\n",
    "    \n",
    "    def get_idf(self, term):\n",
    "        num_term_categories = 0\n",
    "        # Term contains a single word\n",
    "        if len(term.split(' ')) == 1: \n",
    "            for category in self.categories:\n",
    "                is_present = False\n",
    "                for term in self.category_terms[category]:\n",
    "                    for term_word in term.split(' '):\n",
    "                        if word == term_word:\n",
    "                            num_term_categories += 1\n",
    "                            is_present = True\n",
    "                            break\n",
    "                    if (is_present):\n",
    "                        break\n",
    "        else: # Term contains multiple words\n",
    "            num_term_categories = self.get_term_categories(term)\n",
    "            \n",
    "        if num_term_categories == 0:\n",
    "            raise Exception('IDF called for a term not present in Category Matcher')\n",
    "        num_categories = len(self.categories)\n",
    "        return np.log(num_categories / num_term_categories)\n",
    "    \n",
    "    def get_tf_idf(self, category, word):\n",
    "        return self.get_tf(category, word) * self.get_idf() \n",
    "        \n",
    "    def get_category_vector(self, term):\n",
    "        \"\"\" Returns a category vector representation for give term.\n",
    "        Attributes\n",
    "            term -- Word for which we want to category vector\n",
    "        Returns\n",
    "            Vector correponding to categories of given term.\n",
    "        \"\"\"\n",
    "        term_categories = self.get_term_categories(term)\n",
    "        category_vector = [category in term_categories for category in sorted(self.categories)]\n",
    "        return np.array(category_vector, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CategoryPredictor(object):\n",
    "    \"\"\" Categorry Predictor for documents. Learnt from training data \"\"\"\n",
    "    # TODO: Retain high level structure\n",
    "    def __init__(self, category_file, data_dir):\n",
    "        self.category_matcher_ = CategoryMatcher(category_file, data_dir)\n",
    "        self.category_matcher_.build()\n",
    "        self.raw_labels_ = list(self.category_matcher_.get_categories())\n",
    "        self.label_encoder_ = preprocessing.LabelEncoder()\n",
    "        self.data_dir_ = data_dir\n",
    "        self.X_train_ = []\n",
    "        self.y_train_ = []\n",
    "        \n",
    "    @staticmethod\n",
    "    def read_document(filename):\n",
    "        \"\"\" Read input pdf file and parse text from it \n",
    "            TODO : Some words are not space separated \n",
    "        \"\"\"\n",
    "        if not filename.endswith(\".pdf\"):\n",
    "            raise Exception('Input file is not in pdf format !!')\n",
    "        text = []\n",
    "        pdf = pyPdf.PdfFileReader(open(filename, \"rb\"))\n",
    "        for page in pdf.pages:\n",
    "            raw_text = page.extractText()\n",
    "            raw_text = re.split('\\W+', raw_text)\n",
    "            clean_text = [word.strip() for word in raw_text]\n",
    "            text = text + clean_text\n",
    "        # text = list(text)\n",
    "        return ' '.join(text)\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess(text, remove_stopwords = True):\n",
    "        \"\"\" Function to convert raw text to a sequence of words,\n",
    "            optionally removing stop words. Returns a list of words.\n",
    "        \"\"\"\n",
    "        text = text.strip()\n",
    "        if len(text) == 0:\n",
    "            return text\n",
    "        \n",
    "        # 1. Remove HTML\n",
    "        text = BeautifulSoup(text).get_text()\n",
    "          \n",
    "        # 2. Remove \\n and non-digit characters.\n",
    "        # TODO: Revisit this to see what characters should be kept.\n",
    "        # Probably numbers, -, _ etc\n",
    "        text = re.sub(r\"\\\\n\",\" \", text)\n",
    "        text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "        \n",
    "        # 3. Convert words to lower case and split them\n",
    "        words = text.lower().split()\n",
    "        \n",
    "        # 4. Optionally remove stop words (false by default)\n",
    "        if remove_stopwords:\n",
    "            stops = set(stopwords.words('english'))\n",
    "            words = [w for w in words if not w in stops]\n",
    "        \n",
    "        # 5. Join the words back into one string separated by space, \n",
    "        # and return the result.\n",
    "        return (\" \".join(words))\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        self.label_encoder_.fit(self.raw_labels_)\n",
    "        logger.debug('Training Category Predictor for following categories:\\n{0}'\n",
    "                     .format('\\t'.join(self.raw_labels_)))\n",
    "        \n",
    "        for category_dir in os.listdir(self.data_dir_):\n",
    "            logger.info(\"Processing Category: {0}\".format(category_dir))\n",
    "            documents = os.listdir(os.path.join(self.data_dir_, category_dir))\n",
    "            for document in documents:\n",
    "                raw_text = self.read_document(os.path.join(self.data_dir_, category_dir, document))\n",
    "                # This might not be required if CountVectorizer provides \n",
    "                # all required processing capabilities\n",
    "                clean_text = self.preprocess(raw_text)\n",
    "                self.X_train_.append(clean_text)\n",
    "                self.y_train_.append(self.label_encoder_.transform([category_dir])[0])\n",
    "                \n",
    "    def get_document_vector(self, document_dir, document):\n",
    "        raw_text = self.read_document(os.path.join(self.data_dir_, category_dir, document))\n",
    "        # Consider using count vectorizer for this.\n",
    "        clean_text = self.preprocess(raw_text)\n",
    "                \n",
    "    def train_model(self):\n",
    "        pipeline = Pipeline([\n",
    "            ('vect', TfidfVectorizer(analyzer='word', stop_words='english',\n",
    "                                     strip_accents='unicode', ngram_range=(1,2))),\n",
    "            ('clf', SGDClassifier(penalty='l2'))\n",
    "        ])\n",
    "\n",
    "        # Uncommenting more parameters will give better exploring power but will\n",
    "        # increase processing time in a combinatorial way\n",
    "        parameters = {\n",
    "            'vect__max_df': (0.5, 0.75, 1.0),\n",
    "            #'vect__max_features': (None, 5000, 10000, 50000),\n",
    "            #'tfidf__use_idf': (True, False),\n",
    "            #'tfidf__norm': ('l1', 'l2'),\n",
    "            'clf__alpha': (0.00001, 0.000001),\n",
    "#             'clf__penalty': ('l2', 'elasticnet'),\n",
    "            #'clf__n_iter': (10, 50, 80),\n",
    "        }\n",
    "                \n",
    "        # Find the best parameters for both the feature extraction and the\n",
    "        # classifier\n",
    "        grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=0)\n",
    "\n",
    "        print(\"Performing grid search...\")\n",
    "        print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "        print(\"parameters:\")\n",
    "        pprint(parameters)\n",
    "        t0 = time()\n",
    "        grid_search.fit(self.X_train_, self.y_train_)\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        print()\n",
    "\n",
    "        print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "        print(\"Best parameters set:\")\n",
    "        best_parameters = grid_search.best_estimator_.get_params()\n",
    "        for param_name in sorted(parameters.keys()):\n",
    "            print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    category_matcher = CategoryMatcher(CATEGORY_FILE, DATA_DIR)\n",
    "    category_matcher.build()\n",
    "#     with open(EXPERIMENT_FILE, 'rb') as f:\n",
    "#         for line in f:\n",
    "#             for term in line.split(' '):\n",
    "#                 categories = category_matcher.get_term_categories(term)\n",
    "#                 logger.debug(term, len(categories))\n",
    "#     category_predictor = CategoryPredictor(CATEGORY_FILE, DATA_DIR)\n",
    "#     category_predictor.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'centrifuge': set(['centrifuge']), 'force': set(['centrifuge', 'earthquake']), 'pressure': set(['centrifuge']), 'magnitude': set(['earthquake']), 'rotator': set(['centrifuge']), 'centrifuge pump': set(['centrifuge']), 'richter': set(['earthquake']), 'earthquake': set(['earthquake'])}\n"
     ]
    }
   ],
   "source": [
    "print(category_matcher.term_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing Category: TriaxialLabData\n",
      "('Device Configuration File- Version 1.txt', ' is not a pdf. Skipping')\n",
      "('Experimental Methods.docx', ' is not a pdf. Skipping')\n",
      "INFO:root:Processing Document: Experimental Methods.pdf\n",
      "('\\n\\n\\n\\nProcessing paragraph: ', [u'11  The back pressure is controlled by ', u'the computer and kept constant at the initial value after ', u'consolidation, which is usually 100 kPa. Note that shear is started under drained loading ', u''])\n",
      "('\\nProcessing Line: ', u'11  The back pressure is controlled by ')\n"
     ]
    }
   ],
   "source": [
    "category_matcher.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "category_matcher.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "category_matcher.category_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "category_matcher.term_categories"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
