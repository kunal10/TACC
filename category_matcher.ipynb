{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import logging\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PyPDF2 as pyPdf\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim import models\n",
    "from nltk.corpus import stopwords\n",
    "# Needed only if you want to remove stop words.\n",
    "# nltk.download()\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configurable parameters\n",
    "CATEGORY_FILE = 'categories'\n",
    "EXPERIMENT_FILE = 'e1'\n",
    "DATA_DIR = './data'\n",
    "\n",
    "# Logging level\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CategoryMatcher(object):\n",
    "    \"\"\" Category matcher built from existing categories file.\n",
    "        Can be used to match categories for terms.\n",
    "        TODO : Handle variations like phrases, upper/lower case, typos etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, category_file):\n",
    "        self.category_file = category_file\n",
    "        self.term_categories = {}\n",
    "        self.categories = set([])\n",
    "        self.is_built = False\n",
    "\n",
    "    def get_categories(self):\n",
    "        return self.categories\n",
    "    \n",
    "    def build(self):\n",
    "        \"\"\" Parses the category file and builds a set of categories for each term.\n",
    "        \n",
    "            Assumes following format for category file:\n",
    "            Category1 Word1 Word2 ...\n",
    "            Category2 Word1 Word2 ...\n",
    "            .\n",
    "            .\n",
    "            .\n",
    "            \n",
    "            NOTE: One term can have multiple categories\n",
    "        \"\"\"\n",
    "        with open(self.category_file, 'rb') as f:\n",
    "            for line in f:\n",
    "                terms = line.split(' ')\n",
    "                category = terms[0]\n",
    "                if category not in self.categories:\n",
    "                    self.categories.add(category)\n",
    "                for term in terms[1:0]:\n",
    "                    if term not in self.term_categories:\n",
    "                        logger.debug('Term not present in dict: {0}'.format(term))\n",
    "                        self.term_categories[term] = set([category])\n",
    "                    else:\n",
    "                        logger.debug('Adding new category for term: {0}'.format(term))\n",
    "                        self.term_categories[term].add(category)\n",
    "        self.is_built = True\n",
    "                        \n",
    "    def get_term_categories(self, term):\n",
    "        \"\"\" Returns set of categories for given term\n",
    "        Attributes\n",
    "            term -- Word for which we want to retrieve categories.\n",
    "        Returns\n",
    "            Set of categories given term belongs to.\n",
    "        \"\"\"\n",
    "        if not self.is_built:\n",
    "            raise Exception('Category Matcher not built !!')\n",
    "        if term in self.term_categories:\n",
    "            return self.term_categories[term]\n",
    "        # Return empty set if term categories are not present.\n",
    "        logger.debug('\"{0}\" not present. Returning empty set'.format(term))\n",
    "        return set([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CategoryPredictor(object):\n",
    "    \"\"\" Categorry Predictor for documents. Learnt from training data \"\"\"\n",
    "    \n",
    "    def __init__(self, category_file, data_dir):\n",
    "        self.category_matcher_ = CategoryMatcher(category_file)\n",
    "        self.category_matcher_.build()\n",
    "        self.raw_labels_ = list(self.category_matcher_.get_categories())\n",
    "        self.label_encoder_ = preprocessing.LabelEncoder()\n",
    "        self.data_dir_ = data_dir\n",
    "        self.X_train_ = []\n",
    "        self.y_train_ = []\n",
    "        \n",
    "    @staticmethod\n",
    "    def read_document(filename):\n",
    "        \"\"\" Read input pdf file and parse text from it \n",
    "            TODO : Some words are not space separated \n",
    "        \"\"\"\n",
    "        if not filename.endswith(\".pdf\"):\n",
    "            raise Exception('Input file is not in pdf format !!')\n",
    "        text = []\n",
    "        pdf = pyPdf.PdfFileReader(open(filename, \"rb\"))\n",
    "        for page in pdf.pages:\n",
    "            raw_text = page.extractText()\n",
    "            raw_text = re.split('\\W+', raw_text)\n",
    "            clean_text = [word.strip() for word in raw_text]\n",
    "            text = text + clean_text\n",
    "        # text = list(text)\n",
    "        return ' '.join(text)\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess(text, remove_stopwords = True):\n",
    "        \"\"\" Function to convert raw text to a sequence of words,\n",
    "            optionally removing stop words. Returns a list of words.\n",
    "        \"\"\"\n",
    "        text = text.strip()\n",
    "        if len(text) == 0:\n",
    "            return text\n",
    "        \n",
    "        # 1. Remove HTML\n",
    "        text = BeautifulSoup(text).get_text()\n",
    "          \n",
    "        # 2. Remove \\n and non-digit characters\n",
    "        text = re.sub(r\"\\\\n\",\" \", text)\n",
    "        text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "        \n",
    "        # 3. Convert words to lower case and split them\n",
    "        words = text.lower().split()\n",
    "        \n",
    "        # 4. Optionally remove stop words (false by default)\n",
    "        if remove_stopwords:\n",
    "            stops = set(stopwords.words('english'))\n",
    "            words = [w for w in words if not w in stops]\n",
    "        \n",
    "        # 5. Join the words back into one string separated by space, \n",
    "        # and return the result.\n",
    "        return (\" \".join(words))\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        self.label_encoder_.fit(self.raw_labels_)\n",
    "        logger.debug('Training Category Predictor for following categories:\\n{0}'\n",
    "                     .format('\\t'.join(self.raw_labels_)))\n",
    "        \n",
    "        for category_dir in os.listdir(self.data_dir_):\n",
    "            logger.info(\"Processing Category: {0}\".format(category_dir))\n",
    "            documents = os.listdir(os.path.join(self.data_dir_, category_dir))\n",
    "            for document in documents:\n",
    "                raw_text = self.read_document(os.path.join(self.data_dir_, category_dir, document))\n",
    "                # This might not be required if CountVectorizer provides \n",
    "                # all required processing capabilities\n",
    "                clean_text = self.preprocess(raw_text)\n",
    "                self.X_train_.append(clean_text)\n",
    "                self.y_train_.append(self.label_encoder_.transform([category_dir])[0])\n",
    "                \n",
    "    def train_model(self):\n",
    "        pipeline = Pipeline([\n",
    "            ('vect', TfidfVectorizer(analyzer='word', stop_words='english',\n",
    "                                     strip_accents='unicode', ngram_range=(1,2))),\n",
    "            ('clf', SGDClassifier(penalty='l2'))\n",
    "        ])\n",
    "\n",
    "        # Uncommenting more parameters will give better exploring power but will\n",
    "        # increase processing time in a combinatorial way\n",
    "        parameters = {\n",
    "            'vect__max_df': (0.5, 0.75, 1.0),\n",
    "            #'vect__max_features': (None, 5000, 10000, 50000),\n",
    "            #'tfidf__use_idf': (True, False),\n",
    "            #'tfidf__norm': ('l1', 'l2'),\n",
    "            'clf__alpha': (0.00001, 0.000001),\n",
    "#             'clf__penalty': ('l2', 'elasticnet'),\n",
    "            #'clf__n_iter': (10, 50, 80),\n",
    "        }\n",
    "                \n",
    "        # Find the best parameters for both the feature extraction and the\n",
    "        # classifier\n",
    "        grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=0)\n",
    "\n",
    "        print(\"Performing grid search...\")\n",
    "        print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "        print(\"parameters:\")\n",
    "        pprint(parameters)\n",
    "        t0 = time()\n",
    "        grid_search.fit(self.X_train_, self.y_train_)\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        print()\n",
    "\n",
    "        print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "        print(\"Best parameters set:\")\n",
    "        best_parameters = grid_search.best_estimator_.get_params()\n",
    "        for param_name in sorted(parameters.keys()):\n",
    "            print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Training Category Predictor for following categories:\n",
      "centrifuge\tearthquake\n",
      "INFO:root:Processing Category: centrifuge\n",
      "INFO:root:Processing Category: earthquake\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    category_matcher = CategoryMatcher(CATEGORY_FILE)\n",
    "    category_matcher.build()\n",
    "    with open(EXPERIMENT_FILE, 'rb') as f:\n",
    "        for line in f:\n",
    "            for term in line.split(' '):\n",
    "                categories = category_matcher.get_term_categories(term)\n",
    "                logger.debug(term, categories)\n",
    "                \n",
    "if __name__ == '__main__':\n",
    "    category_predictor = CategoryPredictor(CATEGORY_FILE, DATA_DIR)\n",
    "    category_predictor.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "('pipeline:', ['vect', 'clf'])\n",
      "parameters:\n",
      "{'clf__alpha': (1e-05, 1e-06), 'vect__max_df': (0.5, 0.75, 1.0)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: The least populated class in y has only 2 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=3. [_split.py:579]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.477s\n",
      "()\n",
      "Best score: 1.000\n",
      "Best parameters set:\n",
      "\tclf__alpha: 1e-05\n",
      "\tvect__max_df: 0.5\n"
     ]
    }
   ],
   "source": [
    "category_predictor.train_model()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
